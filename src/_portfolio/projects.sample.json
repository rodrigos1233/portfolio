[
  {
    "id": "realtime-collab-engine",
    "title": "Realtime Collaboration Engine",
    "tagline": "CRDT-based multiplayer editing infrastructure for web applications",
    "status": "live",
    "visibility": "private",
    "tags": [
      "infrastructure",
      "real-time",
      "sdk"
    ],
    "stack": [
      "TypeScript",
      "Yjs",
      "WebSocket",
      "WebRTC",
      "PostgreSQL",
      "Redis"
    ],
    "role": "team",
    "timeframe": {
      "start": "2023-06",
      "end": null
    },
    "featured": true,
    "origin": "professional",
    "toc": true,
    "metrics": {
      "users": 12000,
      "requests_per_day": 450000,
      "latency_ms_p95": 48,
      "uptime": "99.97%",
      "notes": "Measured across two production deployments"
    },
    "links": {
      "live": "https://example.com/collab",
      "docs": "https://docs.example.com/collab"
    },
    "attribution": {
      "ownership": "Employer Corp — shared with permission",
      "context": "Built as part of the core platform team at Employer Corp to enable multiplayer features across all product lines.",
      "my_role": "Lead engineer — designed the CRDT sync layer, WebRTC fallback, and conflict resolution strategy.",
      "team_size": 4,
      "permissions": {
        "code_public": false,
        "screenshots_public": true,
        "discussion_level": "architecture"
      }
    },
    "markdown": "## Context / Problem\n\nBuilding multiplayer features from scratch is complex and error-prone. Every team in the company was independently implementing cursor sharing, presence, and conflict resolution — leading to inconsistent behavior and duplicated effort.\n\n## Solution\n\nBuilt a lightweight CRDT engine using Yjs as the foundation, wrapped in a framework-agnostic SDK. The engine handles:\n\n- **Awareness protocol** — cursor positions, selections, and user presence\n- **Document sync** — real-time state synchronization via WebSocket with WebRTC peer-to-peer fallback\n- **Conflict resolution** — automatic merge with deterministic ordering\n- **Persistence** — incremental snapshots to PostgreSQL with Redis pub/sub for cross-server coordination\n\n## Architecture\n\n```mermaid\narchitecture-beta\n    group clients(internet)[Clients]\n    group backend(cloud)[Backend]\n\n    service clientA(internet)[Client A] in clients\n    service clientB(internet)[Client B] in clients\n    service idbA(disk)[IndexedDB] in clients\n    service idbB(disk)[IndexedDB] in clients\n\n    service ws(server)[WebSocket Server] in backend\n    service pg(database)[PostgreSQL] in backend\n    service redis(database)[Redis] in backend\n\n    clientA:R --> L:ws\n    clientB:L --> R:ws\n    clientA:B --> T:idbA\n    clientB:B --> T:idbB\n    ws:B --> T:pg\n    ws:B --> T:redis\n```\n\nThe sync server maintains a lightweight in-memory document state and broadcasts updates. Clients use IndexedDB for offline support and optimistic updates.\n\n## Current State\n\nDeployed in production for two commercial products, handling ~450K requests/day across 12K active users. P95 latency sits at 48ms for sync operations. The SDK is used by three internal teams."
  },
  {
    "id": "spectral-analyzer",
    "title": "Spectral Audio Analyzer",
    "tagline": "Real-time FFT visualization and frequency analysis tool for the browser",
    "status": "experimental",
    "visibility": "public",
    "tags": [
      "audio",
      "visualization",
      "web-api"
    ],
    "stack": [
      "TypeScript",
      "Web Audio API",
      "Canvas API",
      "Vite",
      "Rust",
      "WebAssembly"
    ],
    "role": "solo",
    "timeframe": {
      "start": "2024-09"
    },
    "featured": false,
    "origin": "personal",
    "links": {
      "repo": "https://github.com/example/spectral-analyzer",
      "demo": "https://spectral.example.dev"
    },
    "media": {
      "cover_image": "https://images.unsplash.com/photo-1689176796800-ab67fbf93329?w=1200&auto=format&fit=crop&q=80",
      "gallery": []
    },
    "markdown": "## Context / Problem\n\nExisting browser-based audio analyzers are either too simplistic (basic waveform display) or too heavy (full DAW-like interfaces). I wanted a focused tool for quickly inspecting frequency content during audio development work.\n\n## Solution\n\nA minimal spectral analysis tool that captures microphone or system audio input and renders a real-time FFT spectrogram. The heavy DSP work runs in a Rust/WASM module for performance.\n\n### Features\n\n- **Live spectrogram** — scrolling frequency×time heatmap with configurable color maps\n- **Peak detection** — automatic identification of dominant frequencies\n- **Export** — snapshot current analysis to PNG or CSV\n- **Configurable FFT** — window size (256–8192), overlap, and windowing function selection\n\n## Architecture\n\n```mermaid\narchitecture-beta\n    group input(cloud)[Audio Input]\n    group processing(cloud)[Processing]\n    group output(cloud)[Output]\n\n    service mic(internet)[Mic Input] in input\n    service webAudio(server)[Web Audio API] in input\n    service analyser(server)[AnalyserNode] in processing\n    service fft(server)[WASM FFT Module] in processing\n    service canvas(disk)[Canvas Renderer] in output\n\n    mic:R --> L:webAudio\n    webAudio:R --> L:analyser\n    analyser:B --> T:fft\n    fft:R --> L:canvas\n```\n\nThe Web Audio API provides raw PCM data, which gets passed to a Rust module compiled to WASM for the FFT computation. Results are rendered frame-by-frame onto a Canvas element.\n\n## Current State\n\nFunctional prototype. FFT processing works well, but the Canvas renderer could be optimized — considering migrating to WebGL for the spectrogram display. Peak detection algorithm needs tuning for noisy environments."
  },
  {
    "id": "markdown-graph",
    "title": "Markdown Knowledge Graph",
    "tagline": "A tool that extracts and visualizes relationships between linked markdown notes",
    "status": "archived",
    "visibility": "public",
    "tags": [
      "developer-tools",
      "graph",
      "cli"
    ],
    "stack": [
      "Python",
      "NetworkX",
      "D3.js",
      "Click",
      "FastAPI"
    ],
    "role": "solo",
    "timeframe": {
      "start": "2023-01",
      "end": "2023-08"
    },
    "featured": false,
    "origin": "personal",
    "links": {
      "repo": "https://github.com/example/markdown-graph"
    },
    "markdown": "## Context / Problem\n\nI kept a large collection of interlinked markdown notes (wiki-style `[[double bracket]]` links) and wanted to understand the structure — which topics clustered together, which notes were orphaned, and what the overall knowledge topology looked like.\n\n## Solution\n\nA CLI tool and lightweight web viewer that parses a directory of markdown files, extracts `[[wikilinks]]` and standard markdown links, builds a directed graph, and serves an interactive force-directed visualization.\n\n### Features\n\n- **Link extraction** — supports `[[wikilinks]]`, `[text](url)`, and frontmatter `related:` fields\n- **Graph analysis** — PageRank, clustering coefficient, orphan detection\n- **Interactive viewer** — D3.js force-directed graph with zoom, search, and filtering\n- **CLI interface** — `mdgraph analyze ./notes`, `mdgraph serve`, `mdgraph stats`\n\n## Architecture\n\n```mermaid\narchitecture-beta\n    group ingestion(cloud)[Ingestion]\n    group analysis(cloud)[Analysis]\n    group presentation(cloud)[Presentation]\n\n    service files(disk)[Markdown Files] in ingestion\n    service parser(server)[Parser] in ingestion\n    service graph(server)[NetworkX Graph] in analysis\n    service analytics(server)[PageRank Clusters] in analysis\n    service api(server)[FastAPI Server] in presentation\n    service frontend(internet)[D3 Frontend] in presentation\n\n    files:R --> L:parser\n    parser:R --> L:graph\n    graph:B --> T:analytics\n    graph:R --> L:api\n    api:R --> L:frontend\n```\n\nThe parser handles multiple link formats and builds a NetworkX directed graph. Analysis runs server-side and exposes results via a REST API that the D3.js frontend consumes.\n\n## Current State\n\nArchived after I moved to Obsidian, which has built-in graph features. The core parsing and analysis code still works and could be extracted as a library. The D3.js visualization was the most polished part — smooth animations and good performance up to ~2000 nodes."
  },
  {
    "id": "portfolio",
    "title": "Portfolio",
    "tagline": "A data-driven portfolio site that assembles itself from distributed project repos at build time",
    "status": "live",
    "visibility": "public",
    "tags": [
      "web",
      "developer-tools",
      "open-source"
    ],
    "stack": [
      "TypeScript",
      "React",
      "Vite",
      "Tailwind CSS",
      "Cloudflare Workers"
    ],
    "role": "solo",
    "featured": false,
    "origin": "personal",
    "links": {
      "repo": "https://github.com/rodrigos1233/portfolio"
    },
    "toc": false,
    "markdown": "## Context / Problem\n\nMost portfolio sites are monoliths — every project's description, metadata, and media lives in the portfolio repo itself. This creates a maintenance problem: project details go stale because updating them means context-switching into a separate repo, and there's no validation that the content is complete or well-structured.\n\nI wanted each project to own its own presentation, with the portfolio site acting as an aggregator that assembles everything at build time.\n\n## Solution\n\nA React site backed by a build-time data pipeline. Each project repo contains a `PORTFOLIO_PRESENTATION.md` file with YAML front matter for structured metadata and markdown for the narrative body. A prebuild script fetches these files from GitHub, validates them against a JSON Schema, and writes the collected data for Vite to bundle.\n\n### Key design decisions\n\n- **Decoupled data ownership** — projects describe themselves in their own repos, not in the portfolio repo. Updating a project's description is a commit in that project, not here.\n- **Schema-validated front matter** — AJV validates every presentation against a strict JSON Schema at build time. Missing fields, bad types, or unknown tags fail the build immediately with clear error messages.\n- **Two data modes** — live mode fetches from GitHub (requires a token), mock mode copies sample data for local development with zero setup.\n- **Lazy-loaded Mermaid diagrams** — architecture diagrams are code-split and only load when a project actually uses them. Render failures show the source code and error instead of a blank space.\n- **Automated rebuilds** — project repos trigger a portfolio rebuild via GitHub Actions repository dispatch when their presentation file changes.\n\n## Architecture\n\n```mermaid\narchitecture-beta\n    group sources(internet)[Project Repos]\n    group pipeline(cloud)[Build Pipeline]\n    group site(cloud)[Static Site]\n\n    service repo1(disk)[Repo A] in sources\n    service repo2(disk)[Repo B] in sources\n    service collector(server)[Collect Script] in pipeline\n    service validator(server)[Schema Validator] in pipeline\n    service vite(server)[Vite Build] in pipeline\n    service cdn(internet)[Cloudflare Workers] in site\n\n    repo1:R --> L:collector\n    repo2:R --> L:collector\n    collector:R --> L:validator\n    validator:R --> L:vite\n    vite:R --> L:cdn\n```\n\nThe collect script fetches each presentation via the GitHub API, parses YAML front matter with gray-matter, and validates it with AJV. Valid projects are written to a JSON file that Vite bundles into the React app. The static output deploys to Cloudflare Workers with SPA fallback routing.\n\n## Current State\n\nLive and serving the projects you see on this site. The system works well for a small number of projects. If the project count grows significantly, the build-time fetch approach may need caching or incremental builds to keep deploy times reasonable."
  },
  {
    "id": "logistics-dashboard",
    "title": "Logistics Dashboard",
    "tagline": "Real-time fleet tracking and route optimization dashboard for a logistics company",
    "status": "wip",
    "visibility": "private",
    "tags": [
      "web",
      "visualization",
      "data"
    ],
    "stack": [
      "TypeScript",
      "Next.js",
      "Mapbox GL",
      "PostgreSQL",
      "tRPC"
    ],
    "role": "client",
    "timeframe": {
      "start": "2025-03",
      "end": null
    },
    "featured": false,
    "origin": "professional",
    "links": {
      "post": "https://example.com/blog/logistics-dashboard",
      "video": "https://example.com/videos/dashboard-demo"
    },
    "media": {
      "cover_image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=1200&auto=format&fit=crop&q=80",
      "gallery": [
        "https://images.unsplash.com/photo-1759272840712-c7e5ea852367?w=1200&auto=format&fit=crop&q=80",
        "https://images.unsplash.com/photo-1492168732976-2676c584c675?w=1200&auto=format&fit=crop&q=80"
      ]
    },
    "metrics": {
      "users": 85,
      "requests_per_day": null,
      "latency_ms_p95": null,
      "uptime": null,
      "notes": "Internal tool — 85 dispatchers across 3 regional offices"
    },
    "attribution": {
      "ownership": "Client Co — shared with permission for portfolio use",
      "context": "Contracted to design and build a fleet management dashboard replacing a legacy spreadsheet-based workflow.",
      "my_role": "Sole developer — full-stack implementation from database schema to UI.",
      "team_size": 1,
      "permissions": {
        "code_public": false,
        "screenshots_public": false,
        "discussion_level": "general"
      }
    },
    "markdown": "## Context / Problem\n\nA mid-size logistics company managed fleet dispatch through spreadsheets and phone calls. Dispatchers had no visibility into real-time vehicle positions, and route planning was done manually using paper maps and experience.\n\n## Solution\n\nA web-based dashboard that ingests GPS telemetry from fleet vehicles and displays them on an interactive map. Key features:\n\n- **Live map** — real-time vehicle positions with status indicators (en route, idle, delayed)\n- **Route optimization** — suggests optimal delivery sequences based on time windows and vehicle capacity\n- **Alerts** — automated notifications for delays, geofence violations, and maintenance schedules\n- **Reporting** — daily and weekly summaries of fleet utilization and delivery performance\n\n## Current State\n\nIn active development. The live map and basic route display are functional. Route optimization and alerting are next."
  }
]